{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqv7jtoB45q+dBL8jZ4+xf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snow-white2024/AIFFEL_quest_cr/blob/master/10_%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8%EB%A1%9C_%EB%A7%8C%EB%93%9C%EB%8A%94_%EB%8C%80%ED%99%94%ED%98%95_%EC%B1%97%EB%B4%87_%5B%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. 데이터 수집하기"
      ],
      "metadata": {
        "id": "uvb_iU-3FyZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LhIqn4MEumF",
        "outputId": "dcee4ec1-fd15-4f5f-d167-dadf5595fb6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: (11823, 3)\n",
            "                 Q            A  label\n",
            "0           12시 땡!   하루가 또 가네요.      0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.      0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "\n",
        "# 데이터 다운로드 (GitHub에서 직접 가져오기)\n",
        "dataset_url = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
        "dataset_path = \"ChatbotData.csv\"\n",
        "\n",
        "# 파일 다운로드\n",
        "urllib.request.urlretrieve(dataset_url, dataset_path)\n",
        "\n",
        "# 데이터 불러오기\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# 데이터 크기 및 샘플 확인\n",
        "print(f\"데이터 크기: {df.shape}\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. 데이터 전처리하기"
      ],
      "metadata": {
        "id": "zA-pgCH1F8-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 전처리 함수 정의\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()  # 소문자 변환 및 공백 제거\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)  # 구두점과 단어 분리\n",
        "    sentence = re.sub(r\"[^ㄱ-ㅎ가-힣0-9a-zA-Z?.!,]+\", \" \", sentence)  # 특수문자 제거\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence)  # 다중 공백 제거\n",
        "    return sentence\n",
        "\n",
        "# 데이터 전처리 적용\n",
        "df['Q'] = df['Q'].apply(preprocess_sentence)\n",
        "df['A'] = df['A'].apply(preprocess_sentence)\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04_x72tNGAjM",
        "outputId": "bd31302d-ef40-4ec3-81b3-ae17490a5f5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Q              A  label\n",
            "0         12시 땡 !    하루가 또 가네요 .       0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다 .       0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠 .       0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠 .       0\n",
            "4          ppl 심하네   눈살이 찌푸려지죠 .       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. SubwordTextEncoder 사용하기"
      ],
      "metadata": {
        "id": "-CzZ6tOSGFCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf  # TensorFlow 라이브러리 임포트\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# 질문과 답변 데이터 리스트\n",
        "questions = df['Q'].tolist()\n",
        "answers = df['A'].tolist()\n",
        "\n",
        "# 토크나이저 훈련 (SubwordTextEncoder 사용)\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    questions + answers, target_vocab_size=2**13)\n",
        "\n",
        "# 시작/종료 토큰 정의\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2  # 시작/종료 토큰 포함\n",
        "\n",
        "# 정수 인코딩 및 패딩\n",
        "MAX_LENGTH = 40  # 최대 문장 길이 설정\n",
        "\n",
        "def tokenize_and_filter(sentences):\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tokens = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
        "        if len(tokens) <= MAX_LENGTH:\n",
        "            tokenized_sentences.append(tokens)\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_sentences, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "# 질문 및 답변 토큰화 및 패딩\n",
        "questions = tokenize_and_filter(questions)\n",
        "answers = tokenize_and_filter(answers)\n",
        "\n",
        "print(f\"단어장 크기: {VOCAB_SIZE}\")\n",
        "print(f\"토큰화된 질문 샘플: {questions[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNp-a9ZrGETT",
        "outputId": "f668f3b6-8d85-4bec-e16c-2a524948aa3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어장 크기: 8168\n",
            "토큰화된 질문 샘플: [8166 7905 4199 3051   38 8167    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. 트랜스포머 모델 구성하기"
      ],
      "metadata": {
        "id": "Fm9uViQtGL09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 스케일드 닷 프로덕트 어텐션 함수\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output\n",
        "\n",
        "# 멀티 헤드 어텐션\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.split_heads(self.query_dense(q), batch_size)\n",
        "        k = self.split_heads(self.key_dense(k), batch_size)\n",
        "        v = self.split_heads(self.value_dense(v), batch_size)\n",
        "\n",
        "        attention_output = scaled_dot_product_attention(q, k, v, mask)\n",
        "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(concat_attention)\n",
        "        return output\n",
        "\n",
        "# 트랜스포머 모델 구성\n",
        "def build_transformer(vocab_size, num_layers=2, d_model=128, num_heads=8, dff=512):\n",
        "    inputs = tf.keras.layers.Input(shape=(MAX_LENGTH,))\n",
        "    embedding = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    x = embedding\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        x = MultiHeadAttention(d_model, num_heads)(x, x, x, None)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# 모델 생성 및 컴파일\n",
        "transformer = build_transformer(VOCAB_SIZE)\n",
        "transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "transformer.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "iUg2DDVCIdId",
        "outputId": "c225bb8f-6167-430e-a7df-eec9c3924e37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │      \u001b[38;5;34m1,045,504\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m66,048\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m66,048\u001b[0m │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │                        │                │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8168\u001b[0m)     │      \u001b[38;5;34m1,053,672\u001b[0m │ multi_head_attention_… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,045,504</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │                        │                │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8168</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,053,672</span> │ multi_head_attention_… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,231,272\u001b[0m (8.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,231,272</span> (8.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,231,272\u001b[0m (8.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,231,272</span> (8.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4-1. 모델 학습하기"
      ],
      "metadata": {
        "id": "DooHeGlANgTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1. 데이터셋 준비\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# TensorFlow Dataset으로 변환\n",
        "dataset = tf.data.Dataset.from_tensor_slices((questions, answers))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Step 2. 콜백 함수 정의 (모델 체크포인트 저장)\n",
        "checkpoint_path = \"transformer_chatbot.weights.h5\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    monitor=\"loss\",\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 3. 모델 학습 (EPOCHS 지정)\n",
        "EPOCHS = 50  # 원하는 만큼 조정 가능\n",
        "\n",
        "transformer.fit(\n",
        "    dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n",
        "\n",
        "print(\"✅ 모델 학습 완료!\")\n"
      ],
      "metadata": {
        "id": "F0AQKT4LNoTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01c2efe-8fc3-44a7-f283-4e24abd31a7b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7807 - loss: 3.0142\n",
            "Epoch 1: loss improved from inf to 2.04110, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.7808 - loss: 3.0090\n",
            "Epoch 2/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8059 - loss: 1.6499\n",
            "Epoch 2: loss improved from 2.04110 to 1.66481, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8058 - loss: 1.6502\n",
            "Epoch 3/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8093 - loss: 1.5028\n",
            "Epoch 3: loss improved from 1.66481 to 1.39016, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8095 - loss: 1.5004\n",
            "Epoch 4/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8338 - loss: 1.2558\n",
            "Epoch 4: loss improved from 1.39016 to 1.25287, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8338 - loss: 1.2558\n",
            "Epoch 5/50\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8361 - loss: 1.2225\n",
            "Epoch 5: loss improved from 1.25287 to 1.22398, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8361 - loss: 1.2225\n",
            "Epoch 6/50\n",
            "\u001b[1m181/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8369 - loss: 1.1942\n",
            "Epoch 6: loss improved from 1.22398 to 1.20169, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8369 - loss: 1.1944\n",
            "Epoch 7/50\n",
            "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8368 - loss: 1.1778\n",
            "Epoch 7: loss improved from 1.20169 to 1.18577, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8368 - loss: 1.1780\n",
            "Epoch 8/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8371 - loss: 1.1656\n",
            "Epoch 8: loss improved from 1.18577 to 1.17494, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8371 - loss: 1.1657\n",
            "Epoch 9/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8383 - loss: 1.1483\n",
            "Epoch 9: loss improved from 1.17494 to 1.16318, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8383 - loss: 1.1485\n",
            "Epoch 10/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8384 - loss: 1.1364\n",
            "Epoch 10: loss improved from 1.16318 to 1.14966, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8384 - loss: 1.1367\n",
            "Epoch 11/50\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8392 - loss: 1.1232\n",
            "Epoch 11: loss improved from 1.14966 to 1.13621, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8392 - loss: 1.1233\n",
            "Epoch 12/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8393 - loss: 1.1088\n",
            "Epoch 12: loss improved from 1.13621 to 1.12159, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8393 - loss: 1.1089\n",
            "Epoch 13/50\n",
            "\u001b[1m181/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8403 - loss: 1.0858\n",
            "Epoch 13: loss improved from 1.12159 to 1.10608, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8403 - loss: 1.0864\n",
            "Epoch 14/50\n",
            "\u001b[1m181/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8402 - loss: 1.0753\n",
            "Epoch 14: loss improved from 1.10608 to 1.08975, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8402 - loss: 1.0757\n",
            "Epoch 15/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8415 - loss: 1.0491\n",
            "Epoch 15: loss improved from 1.08975 to 1.06861, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8415 - loss: 1.0493\n",
            "Epoch 16/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8414 - loss: 1.0339\n",
            "Epoch 16: loss improved from 1.06861 to 1.04763, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8414 - loss: 1.0342\n",
            "Epoch 17/50\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8425 - loss: 1.0107\n",
            "Epoch 17: loss improved from 1.04763 to 1.02361, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8425 - loss: 1.0107\n",
            "Epoch 18/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8430 - loss: 0.9843\n",
            "Epoch 18: loss improved from 1.02361 to 0.99879, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8429 - loss: 0.9846\n",
            "Epoch 19/50\n",
            "\u001b[1m181/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8438 - loss: 0.9688\n",
            "Epoch 19: loss improved from 0.99879 to 0.97476, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8438 - loss: 0.9689\n",
            "Epoch 20/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8458 - loss: 0.9287\n",
            "Epoch 20: loss improved from 0.97476 to 0.95007, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8458 - loss: 0.9291\n",
            "Epoch 21/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8476 - loss: 0.9033\n",
            "Epoch 21: loss improved from 0.95007 to 0.92077, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8475 - loss: 0.9037\n",
            "Epoch 22/50\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8488 - loss: 0.8802\n",
            "Epoch 22: loss improved from 0.92077 to 0.89362, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8488 - loss: 0.8803\n",
            "Epoch 23/50\n",
            "\u001b[1m181/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8500 - loss: 0.8577\n",
            "Epoch 23: loss improved from 0.89362 to 0.86703, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8500 - loss: 0.8579\n",
            "Epoch 24/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8542 - loss: 0.8208\n",
            "Epoch 24: loss improved from 0.86703 to 0.83942, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8542 - loss: 0.8210\n",
            "Epoch 25/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8560 - loss: 0.7935\n",
            "Epoch 25: loss improved from 0.83942 to 0.81328, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8560 - loss: 0.7937\n",
            "Epoch 26/50\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8579 - loss: 0.7739\n",
            "Epoch 26: loss improved from 0.81328 to 0.78952, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8579 - loss: 0.7740\n",
            "Epoch 27/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8596 - loss: 0.7557\n",
            "Epoch 27: loss improved from 0.78952 to 0.76831, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8596 - loss: 0.7559\n",
            "Epoch 28/50\n",
            "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8629 - loss: 0.7262\n",
            "Epoch 28: loss improved from 0.76831 to 0.74613, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8629 - loss: 0.7265\n",
            "Epoch 29/50\n",
            "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8633 - loss: 0.7160\n",
            "Epoch 29: loss improved from 0.74613 to 0.72566, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8633 - loss: 0.7162\n",
            "Epoch 30/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8678 - loss: 0.6804\n",
            "Epoch 30: loss improved from 0.72566 to 0.70318, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8677 - loss: 0.6809\n",
            "Epoch 31/50\n",
            "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8703 - loss: 0.6595\n",
            "Epoch 31: loss improved from 0.70318 to 0.68193, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8703 - loss: 0.6599\n",
            "Epoch 32/50\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8706 - loss: 0.6527\n",
            "Epoch 32: loss improved from 0.68193 to 0.66388, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8706 - loss: 0.6528\n",
            "Epoch 33/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8750 - loss: 0.6187\n",
            "Epoch 33: loss improved from 0.66388 to 0.64295, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8749 - loss: 0.6192\n",
            "Epoch 34/50\n",
            "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8747 - loss: 0.6152\n",
            "Epoch 34: loss improved from 0.64295 to 0.63044, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8747 - loss: 0.6155\n",
            "Epoch 35/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8785 - loss: 0.5927\n",
            "Epoch 35: loss improved from 0.63044 to 0.60774, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8785 - loss: 0.5930\n",
            "Epoch 36/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8798 - loss: 0.5810\n",
            "Epoch 36: loss improved from 0.60774 to 0.59344, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8798 - loss: 0.5811\n",
            "Epoch 37/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8814 - loss: 0.5697\n",
            "Epoch 37: loss improved from 0.59344 to 0.58312, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8813 - loss: 0.5700\n",
            "Epoch 38/50\n",
            "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8825 - loss: 0.5566\n",
            "Epoch 38: loss improved from 0.58312 to 0.56423, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8825 - loss: 0.5568\n",
            "Epoch 39/50\n",
            "\u001b[1m182/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8845 - loss: 0.5419\n",
            "Epoch 39: loss improved from 0.56423 to 0.55289, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8845 - loss: 0.5422\n",
            "Epoch 40/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8866 - loss: 0.5320\n",
            "Epoch 40: loss improved from 0.55289 to 0.53814, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8866 - loss: 0.5320\n",
            "Epoch 41/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8892 - loss: 0.5130\n",
            "Epoch 41: loss improved from 0.53814 to 0.52405, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8892 - loss: 0.5131\n",
            "Epoch 42/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8902 - loss: 0.5073\n",
            "Epoch 42: loss improved from 0.52405 to 0.51610, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8902 - loss: 0.5073\n",
            "Epoch 43/50\n",
            "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8927 - loss: 0.4896\n",
            "Epoch 43: loss improved from 0.51610 to 0.50279, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8927 - loss: 0.4898\n",
            "Epoch 44/50\n",
            "\u001b[1m184/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8957 - loss: 0.4730\n",
            "Epoch 44: loss improved from 0.50279 to 0.49001, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8956 - loss: 0.4732\n",
            "Epoch 45/50\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8955 - loss: 0.4722\n",
            "Epoch 45: loss improved from 0.49001 to 0.48083, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8955 - loss: 0.4723\n",
            "Epoch 46/50\n",
            "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8968 - loss: 0.4649\n",
            "Epoch 46: loss improved from 0.48083 to 0.47616, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8967 - loss: 0.4651\n",
            "Epoch 47/50\n",
            "\u001b[1m181/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8980 - loss: 0.4597\n",
            "Epoch 47: loss improved from 0.47616 to 0.46525, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8980 - loss: 0.4599\n",
            "Epoch 48/50\n",
            "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9008 - loss: 0.4406\n",
            "Epoch 48: loss improved from 0.46525 to 0.45307, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9008 - loss: 0.4408\n",
            "Epoch 49/50\n",
            "\u001b[1m181/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9019 - loss: 0.4359\n",
            "Epoch 49: loss improved from 0.45307 to 0.44436, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9019 - loss: 0.4361\n",
            "Epoch 50/50\n",
            "\u001b[1m181/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9057 - loss: 0.4206\n",
            "Epoch 50: loss improved from 0.44436 to 0.43777, saving model to transformer_chatbot.weights.h5\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9056 - loss: 0.4210\n",
            "✅ 모델 학습 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. 모델 평가하기"
      ],
      "metadata": {
        "id": "7U4Yj9joIgmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict(sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    sentence = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
        "    sentence = tf.keras.preprocessing.sequence.pad_sequences([sentence], maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    prediction = transformer.predict(sentence)\n",
        "    predicted_id = np.argmax(prediction[0], axis=-1)\n",
        "\n",
        "    result = tokenizer.decode([i for i in predicted_id if i < tokenizer.vocab_size])\n",
        "    return result\n",
        "\n",
        "# 테스트\n",
        "print(\"챗봇 테스트:\")\n",
        "while True:\n",
        "    q = input(\"User: \")\n",
        "    if q.lower() == \"exit\":\n",
        "        break\n",
        "    print(\"Bot:\", predict(q))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BulxbbgpImjb",
        "outputId": "5057853a-3a77-48c8-a608-8ba92092fe2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "챗봇 테스트:\n",
            "User: 오늘 날씨가 추워.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731ms/step\n",
            "Bot: 크게 마음도 나가바랍니다나가\n",
            "User: 좋은 아침이야.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Bot: 맛나게 좀 할게요할게요할게요\n",
            "User: 너 말투가 왜 그래?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Bot: 저도 나 권권하지  . \n",
            "User: 너 지금 장난하냐\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Bot:  . 재미있나봐요 . \n",
            "User: ?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Bot: 맞기만 \n"
          ]
        }
      ]
    }
  ]
}